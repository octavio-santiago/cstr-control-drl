{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74eb4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "#import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "from gym.core import ActionWrapper\n",
    "import glob\n",
    "\n",
    "import shutil\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "from scipy import interpolate\n",
    "import math\n",
    "from typing import Any, Dict, Union\n",
    "import time\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sim import cstr_model as cstr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c251435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSTREnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        #n_vars = 5 #T,Tc,Ca,Cr,Tr\n",
    "        #n_actions = 1 #dTc\n",
    "        self.Cref_signal = env_config['Cref_signal']\n",
    "        self.selector = env_config['selector']\n",
    "        self.model = env_config['model']\n",
    "\n",
    "        if self.selector == True:\n",
    "            self.action_space = gym.spaces.Discrete(3) ##add number of concepts\n",
    "            self.models = []\n",
    "\n",
    "            for i in self.model:\n",
    "                self.models.append(Policy.from_checkpoint(glob.glob(i+\"/*\")[0])['default_policy'] )\n",
    "\n",
    "            self.model = self.models\n",
    "\n",
    "        else:\n",
    "            self.action_space = gym.spaces.Box(low=np.array([-10.0]), high=np.array([10.0]))\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([200, 200, 0, 0, 200]), high=np.array([500, 500, 12, 12, 500]))   \n",
    "\n",
    "    def reset(self):\n",
    "        noise_percentage = 0\n",
    "        Cref_signal = self.Cref_signal\n",
    "\n",
    "        #initial conditions\n",
    "        Ca0: float = 8.5698 #kmol/m3\n",
    "        T0: float = 311.2639 #K\n",
    "        Tc0: float = 292 #K\n",
    "\n",
    "        self.T = T0\n",
    "        self.Tc = Tc0\n",
    "        self.Ca = Ca0\n",
    "        self.ΔTc = 0\n",
    "\n",
    "        self.cnt = 0\n",
    "\n",
    "        self.Cref_signal = Cref_signal\n",
    "        self.noise_percentage = noise_percentage\n",
    "        if self.noise_percentage > 0:\n",
    "            self.noise_percentage = self.noise_percentage/100\n",
    "\n",
    "        if self.Cref_signal == 2:\n",
    "            self.Cref = 2\n",
    "            self.Tref = 373.1311\n",
    "            self.Ca = 2\n",
    "            self.T = 373.1311\n",
    "        else:\n",
    "            self.Cref = 8.5698\n",
    "            self.Tref = 311.2612\n",
    "\n",
    "        self.rms = 0\n",
    "        self.y_list = []\n",
    "        self.obs = np.array([self.T,self.Tc,self.Ca,self.Cref,self.Tref], dtype=np.float32)\n",
    "        return self.obs\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.Cref_signal == 0:\n",
    "            self.Cref = 0\n",
    "            self.Tref = 0\n",
    "        elif self.Cref_signal == 1: #transition\n",
    "            #update Cref an Tref\n",
    "            time = 90\n",
    "            p1 = 22 \n",
    "            p2 = 74\n",
    "            k = self.cnt+p1\n",
    "            ceq = [8.57,6.9275,5.2850,3.6425,2]\n",
    "            teq = [311.2612,327.9968,341.1084,354.7246,373.1311]\n",
    "            C = interpolate.interp1d([0,p1,p2,time], [8.57,8.57,2,2])\n",
    "            self.Cref = float(C(k))\n",
    "            T_ = interpolate.interp1d([0,p1,p2,time], [311.2612,311.2612,373.1311,373.1311])\n",
    "            self.Tref = float(T_(k))\n",
    "        elif self.Cref_signal == 2: #steady state 1\n",
    "            self.Cref = 2\n",
    "            self.Tref = 373.1311\n",
    "        elif self.Cref_signal == 3: #steady state 2\n",
    "            self.Cref = 8.5698\n",
    "            self.Tref = 311.2612\n",
    "        elif self.Cref_signal == 4: #full sim\n",
    "            k = self.cnt\n",
    "            time = 90\n",
    "            #update Cref an Tref\n",
    "            p1 = 22 \n",
    "            p2 = 74 \n",
    "            ceq = [8.57,6.9275,5.2850,3.6425,2]\n",
    "            teq = [311.2612,327.9968,341.1084,354.7246,373.1311]\n",
    "            C = interpolate.interp1d([0,p1,p2,time], [8.57,8.57,2,2])\n",
    "            self.Cref = float(C(k))\n",
    "            T_ = interpolate.interp1d([0,p1,p2,time], [311.2612,311.2612,373.1311,373.1311])\n",
    "            self.Tref = float(T_(k))\n",
    "\n",
    "        if self.selector == True:\n",
    "            self.ΔTc = self.model[action].compute_single_action(self.obs)[0][0]\n",
    "        else:\n",
    "            self.ΔTc = action\n",
    "\n",
    "        error_var = self.noise_percentage\n",
    "        σ_max1 = error_var * (8.5698 - 2)\n",
    "        σ_max2 = error_var * ( 373.1311 - 311.2612)\n",
    "\n",
    "        σ_Ca = random.uniform(-σ_max1, σ_max1)\n",
    "        σ_T = random.uniform(-σ_max2, σ_max2)\n",
    "        mu = 0\n",
    "\n",
    "        #calling the CSTR python model\n",
    "        sim_model = cstr.CSTRModel(T = self.T, Ca = self.Ca, Tc = self.Tc, ΔTc = self.ΔTc)\n",
    "\n",
    "        #Tc\n",
    "        self.Tc += self.ΔTc\n",
    "\n",
    "        #Tr\n",
    "        self.T = sim_model.T + σ_T\n",
    "\n",
    "        #Ca\n",
    "        self.Ca = sim_model.Ca + σ_Ca\n",
    "        self.y_list.append(self.Ca)\n",
    "\n",
    "        #Increase time\n",
    "        self.cnt += 1\n",
    "\n",
    "        self.rms = math.sqrt( (self.Ca - self.Cref)**2 )\n",
    "\n",
    "        reward = float(1/self.rms)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        #end the simulation\n",
    "        if self.cnt == 90 or self.T >= 400 or (self.Cref_signal == 1 and self.cnt == 68): \n",
    "            done = True\n",
    "\n",
    "        info = {}\n",
    "        return self.obs, reward, done, info\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d20c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 20:18:40,873\tINFO worker.py:1550 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-03-03 20:18:43,097\tWARNING deprecation.py:51 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.CSTREnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.CSTREnv'>').build()` instead. This will raise an error in the future!\n",
      "2023-03-03 20:18:43,098\tINFO algorithm_config.py:2900 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-03-03 20:18:43,140\tINFO algorithm.py:507 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gymnasium\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m   logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m W0303 20:18:49.122229  6888 deprecation.py:506] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gymnasium\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m   logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 2023-03-03 20:18:49,119\tWARNING utils.py:166 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    `reset()` method.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    setting).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    per-agent dict).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 2023-03-03 20:18:49,132\tWARNING env.py:157 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m 2023-03-03 20:18:49,133\tWARNING env.py:167 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m W0303 20:18:49.148229  5208 deprecation.py:506] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m W0303 20:18:49.518229  6888 deprecation.py:323] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17800)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m W0303 20:18:49.529229  5208 deprecation.py:323] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1576)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0303 20:18:50.077264 28764 deprecation.py:506] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W0303 20:18:50.430254 28764 deprecation.py:323] From c:\\users\\octavio\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2023-03-03 20:18:51,069\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 reward 121.62/218.48/1174.42 len  64.33\n",
      "  2 reward 117.16/253.68/1206.04 len  49.25\n",
      "  3 reward 112.01/232.49/1787.09 len  40.83\n",
      "  4 reward 118.96/285.48/3566.35 len  48.12\n",
      "  5 reward 114.33/2041.69/121922.69 len  36.10\n",
      "  6 reward 119.36/456.71/22866.94 len  35.42\n",
      "  7 reward 117.87/247.03/2380.63 len  24.74\n",
      "  8 reward 119.09/281.99/1941.73 len  42.60\n",
      "  9 reward 119.09/339.34/2485.18 len  54.80\n",
      " 10 reward 125.08/328.99/1956.82 len  53.02\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "config = ppo.PPOConfig()#.rollouts(num_rollout_workers=0)\n",
    "#print(config.to_dict()) \n",
    "\n",
    "algo = ppo.PPOTrainer(env=CSTREnv, config={\n",
    "    \"env_config\": {\"Cref_signal\":4, \"selector\":False, \"model\":[]},  # config to pass to env class\n",
    "})\n",
    "\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f}\"\n",
    "for n in range(10):\n",
    "    result = algo.train()\n",
    "    #print(result)\n",
    "\n",
    "    print(s.format(\n",
    "        n+1,\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665c68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
